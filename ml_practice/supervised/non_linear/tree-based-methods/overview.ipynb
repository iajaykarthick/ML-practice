{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Index](../../../../index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree based methods are algorithms that  segement the predictor space into a series of homogeneous, non-overlapping regions and and predict the average or majority outcome for each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Homogeneous regions: the regions have a high degree of similarity or purity in terms of the outcome variable. For example, \n",
    "  * in a classification problem, a homogeneous region would have mostly observations from one class,\n",
    "  * in a regression problem, a homogeneous region would have a low variance of the outcome values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Non-overlapping regions: the regions that do not share any common points.\n",
    "  * Each point in feature space belongs to exactly one region and there is no ambiguity or overlap between the regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Tree-Based Models\n",
    "\n",
    "### Decision Trees (Individual Tree) \n",
    "- **Description**: The simplest form of tree-based models.\n",
    "- **Functionality**: Creates a tree-like model of decisions. Each internal node signifies a test on a feature, branches represent outcomes of these tests, and leaf nodes denote a class label (for classification) or a value (for regression).\n",
    "\n",
    "[decision tree notes](decision_tree/overview.html)\n",
    "\n",
    "### Ensemble Methods\n",
    "- **General Idea**: Improve model performance by combining multiple decision trees.\n",
    "\n",
    "#### Bagging (Bootstrap Aggregating)\n",
    "- **Core Concept**: Reduces variance by averaging multiple decision trees built on bootstrapped datasets.\n",
    "- **Random Forests**:\n",
    "  - **Description**: An ensemble of decision trees, each built on a slightly different data sample.\n",
    "  - **Advantages**: Reduces overfitting, improves accuracy, handles high-dimensional spaces well.\n",
    "\n",
    "#### Boosting\n",
    "- **Principle**: Combine multiple weak learners sequentially to form a strong learner.\n",
    "\n",
    "  ##### AdaBoost (Adaptive Boosting)\n",
    "  - **Methodology**: Adjusts the weight of observations based on the previous classification.\n",
    "  - **Use Case**: Often used in scenarios where data balance is an issue.\n",
    "\n",
    "  ##### Gradient Boosting Machines (GBMs)\n",
    "  - **Approach**: Focuses on minimizing loss function and adding weak learners using gradient descent.\n",
    "  \n",
    "  ##### XGBoost (Extreme Gradient Boosting)\n",
    "  - **Specialty**: Optimized gradient boosting, known for its performance and speed.\n",
    "\n",
    "  ##### LightGBM\n",
    "  - **Characteristic**: Gradient boosting framework that uses tree-based learning with a focus on efficiency.\n",
    "\n",
    "  ##### CatBoost\n",
    "  - **Feature**: An algorithm with built-in handling of categorical variables.\n",
    "\n",
    "### Hybrid Methods\n",
    "- **Concept**: Combine different machine learning algorithms for better predictive performance.\n",
    "\n",
    "  ##### Stacking (Stacked Generalization)\n",
    "  - **Idea**: Integrates predictions from multiple machine learning models using a meta-learner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
