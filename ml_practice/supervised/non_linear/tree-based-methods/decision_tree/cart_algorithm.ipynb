{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to index](../../../../../index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART (Classification and Regression Trees)\n",
    "\n",
    "- **Type**: Supervised Learning Algorithm\n",
    "- **Uses**: Both Classification and Regression tasks\n",
    "- **Structure**: Binary tree (each node splits into two child nodes)\n",
    "\n",
    "### Process\n",
    "1. **Splitting**: Divides the data into subsets based on feature values. \n",
    "2. **Pruning**: Reduces the size of the tree to prevent overfitting.\n",
    "3. **Tree Selection**: Uses cross-validation to choose the optimal tree size.\n",
    "\n",
    "### Features\n",
    "- **Handling Different Types of Data**: Efficiently processes both numerical and categorical data.\n",
    "- **Binary Splits**: Makes decisions based on a single feature at each node.\n",
    "- **Cost Complexity Pruning**: Balances tree depth and model accuracy.\n",
    "\n",
    "### Advantages\n",
    "- **Flexibility**: Can be applied to various types of data and problems.\n",
    "- **Interpretability**: Provides clear visualization and understanding of how decisions are made.\n",
    "\n",
    "### Disadvantages\n",
    "- **Overfitting Risk**: Requires careful tuning and pruning.\n",
    "- **Binary Splits Limitation**: Each split considers only one feature, which might not capture complex relationships.\n",
    "\n",
    "### Applications\n",
    "- Widely used in medical diagnosis, risk assessment, and other areas where clear decision rules are valuable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART Algorithm Steps:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Start with the entire dataset as a single region.\n",
    "   - Designate this region as the root node of the tree.\n",
    "\n",
    "2. **Feature selection and Best Split**:\n",
    "   - Evaluate each feature to determine the best split at each node.\n",
    "   - Use Gini impurity for classification tasks and Mean Squared Error (MSE) for regression tasks by default.\n",
    "   - Itâ€™s possible to configure CART to use entropy instead of Gini impurity for classification.\n",
    "\n",
    "3. **Binary Split**:\n",
    "   - Perform a binary split of the node into two child nodes using the best split criterion.\n",
    "\n",
    "4. **Recursive Partitioning**:\n",
    "   - Recursively apply the splitting process to each child node.\n",
    "   - Continue until a stopping criterion is met, such as maximum tree depth, minimum node size, or no further improvement in the chosen split criterion.\n",
    "\n",
    "5. **Pruning the tree**:\n",
    "   - Prune the fully grown tree using cost-complexity pruning to balance tree depth and model accuracy.\n",
    "   - Remove branches that contribute little to prediction accuracy.\n",
    "\n",
    "6. **Cross validation and tree selection**:\n",
    "   -  Use cross validation to determine the optimal size of the tree.\n",
    "   -  This helps prevent overfitting by avoiding an overly complex tree.\n",
    "\n",
    "7. **Final Model**:\n",
    "   - Use the pruned tree for making predictions.\n",
    "   - For classification, the prediction is the most common class in the leaf node.\n",
    "   - For regression, the prediction is typically the mean of the response values in the leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
