{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID3 (Iterative Dichotomiser 3) algorithm is a classic decision tree algorithm used in machine learning for classification tasks. It was developed by Ross Quinlan in the 1980s. ID3 uses a greedy approach to build a decision tree by recursively partitioning the dataset into smaller subsets based on the attribute that provides the maximum information gain at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 Algorithm Steps:\n",
    "\n",
    "1. **Start**:\n",
    "   - Begin with the entire dataset as the root node.\n",
    "\n",
    "2. **Entropy Calculation**:\n",
    "   - Calculate the entropy of the target variable for the dataset.\n",
    "\n",
    "3. **Information Gain**:\n",
    "   - For each attribute, calculate the information gain.\n",
    "   - Select the attribute with the highest information gain as the decision attribute for the node.\n",
    "\n",
    "4. **Tree Construction**:\n",
    "   - For each unique value of the decision attribute, create a branch using the equality condition.\n",
    "   - Example: If the decision attribute is \"Color\" with values \"Red,\" \"Green,\" and \"Blue,\" create three branches:\n",
    "     - If Color = Red, then follow the Red branch.\n",
    "     - If Color = Green, then follow the Green branch.\n",
    "     - If Color = Blue, then follow the Blue branch.\n",
    "\n",
    "5. **Recursive Splitting**:\n",
    "   - Split the dataset into subsets based on the attribute values.\n",
    "   - Apply the ID3 algorithm recursively to each subset.\n",
    "\n",
    "6. **Termination**:\n",
    "   - Stop if all instances in a subset belong to the same class (pure).\n",
    "   - Stop if there are no more attributes to be selected, but the instances still don't belong to the same class (use majority voting).\n",
    "   - Stop if there are no instances left.\n",
    "\n",
    "7. **Pruning (Optional)**:\n",
    "   - Prune the tree to handle overfitting if necessary.\n",
    "\n",
    "8. **Final Decision Tree**:\n",
    "   - Use the constructed (and possibly pruned) tree to make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: {'a': 'Positive', 'sad': 'Negative', 'excited': 'Positive', 'afraid': 'Negative'}}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "\n",
    "# Sample dataset\n",
    "# Each entry in the dataset is a tuple (text, label)\n",
    "dataset = [\n",
    "    (\"This is a good day\", \"Positive\"),\n",
    "    (\"I feel sad about the news\", \"Negative\"),\n",
    "    (\"I am excited to see you\", \"Positive\"),\n",
    "    (\"I am afraid of the dark\", \"Negative\")\n",
    "]\n",
    "\n",
    "# Tokenize and preprocess the text\n",
    "def preprocess(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Calculate entropy\n",
    "def entropy(subset):\n",
    "    label_counts = defaultdict(int)\n",
    "    for _, label in subset:\n",
    "        label_counts[label] += 1\n",
    "    total = len(subset)\n",
    "    return -sum((count/total) * log2(count/total) for count in label_counts.values() if count/total > 0)\n",
    "\n",
    "# Calculate information gain\n",
    "def information_gain(dataset, partitions):\n",
    "    total = len(dataset)\n",
    "    dataset_entropy = entropy(dataset)\n",
    "    weighted_entropy = sum((len(partition)/total) * entropy(partition) for partition in partitions)\n",
    "    return dataset_entropy - weighted_entropy\n",
    "\n",
    "# ID3 algorithm\n",
    "def id3(dataset, attributes):\n",
    "    labels = [label for _, label in dataset]\n",
    "    if len(set(labels)) == 1:\n",
    "        return labels[0]\n",
    "\n",
    "    if not attributes:\n",
    "        return max(set(labels), key=labels.count)\n",
    "\n",
    "    max_gain = 0\n",
    "    best_attribute = None\n",
    "    best_partitions = None  # Initialize best_partitions\n",
    "\n",
    "    for attribute in attributes:\n",
    "        partitions = defaultdict(list)\n",
    "        for text, label in dataset:\n",
    "            key = preprocess(text)[attribute]\n",
    "            partitions[key].append((text, label))\n",
    "        gain = information_gain(dataset, partitions.values())\n",
    "        if gain > max_gain:\n",
    "            max_gain = gain\n",
    "            best_attribute = attribute\n",
    "            best_partitions = partitions  # Update best_partitions with the best current partitions\n",
    "\n",
    "    tree = {best_attribute: {}}\n",
    "    for attribute_value, subset in best_partitions.items():\n",
    "        subtree = id3(subset, [a for a in attributes if a != best_attribute])\n",
    "        tree[best_attribute][attribute_value] = subtree\n",
    "\n",
    "    return tree\n",
    "\n",
    "# Example usage\n",
    "attributes = list(range(len(preprocess(dataset[0][0])))) # Assuming all texts are tokenized similarly\n",
    "decision_tree = id3(dataset, attributes)\n",
    "print(decision_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
